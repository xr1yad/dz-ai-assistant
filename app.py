import streamlit as st
import requests
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import os
import numpy as np

# ================= Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© =================
st.set_page_config(page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ", layout="wide")

st.title("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§Ù„Ø°ÙƒÙŠ")
st.write("Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø£Ùˆ Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ğŸ‡©ğŸ‡¿")

# Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ù„ØºÙˆÙŠ Ù…Ù† Hugging Face (Ù…Ø¬Ø§Ù†ÙŠ)
import json

# Ø³Ù†Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø¬Ù‡Ø© API Ù…Ù† Hugging Face Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠÙ‹Ø§
HF_API_TOKEN = "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"

def query_huggingface(prompt):
    api_url = "https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct"
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
    payload = {"inputs": prompt, "parameters": {"max_new_tokens": 300, "temperature": 0.6}}
    response = requests.post(api_url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        try:
            return result[0]["generated_text"]
        except Exception:
            return json.dumps(result)
    else:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Hugging Face API: {response.status_code}"

# Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬
DB_DIR = "chroma_db"
os.makedirs(DB_DIR, exist_ok=True)
# Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù†ØµÙˆØµ (embeddings) Ù…Ù† Hugging Face Ù…Ø¬Ø§Ù†Ù‹Ø§
HF_API_TOKEN = "Ø¶Ø¹_Ø±Ù…Ø²_HuggingFace_Ø§Ù„Ø°ÙŠ_Ù†Ø³Ø®ØªÙ‡_Ù…Ù†_Ù‚Ø¨Ù„_Ù‡Ù†Ø§"

def get_embeddings(texts):
    api_url = "https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2"
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
    embeddings = []
    for text in texts:
        payload = {"inputs": text}
        response = requests.post(api_url, headers=headers, json=payload)
        if response.status_code == 200:
            emb = response.json()
            # Ù†Ø£Ø®Ø° Ø§Ù„Ù…ØªÙˆØ³Ø· Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…
            emb_vector = np.mean(emb, axis=0)
            embeddings.append(emb_vector)
        else:
            embeddings.append(np.zeros(384))  # fallback
    return np.array(embeddings).astype("float32")

client = chromadb.Client(Settings(persist_directory=DB_DIR))
try:
    collection = client.get_collection("curriculum")
except:
    collection = client.create_collection("curriculum")

# ================= ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª =================
def search_internet(query):
    url = "https://google.serper.dev/search"
    headers = {"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"}
    payload = {
        "q": f"{query} site:education.gov.dz OR site:onefd.edu.dz OR site:wikipedia.org OR site:britannica.com",
        "num": 5
    }
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        data = response.json()
        results = []
        for item in data.get("organic", []):
            results.append(f"{item.get('title')} - {item.get('link')}")
        return "\n".join(results)
    else:
        return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø¢Ù†."

# ================= ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… =================
user_input = st.chat_input("ğŸ—¨ï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

if user_input:
    with st.chat_message("user"):
        st.write(user_input)

    with st.chat_message("assistant"):
        st.write("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...")

        # 1. Ø§Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©
        q_emb = embed_model.encode([user_input])
        results = collection.query(query_embeddings=q_emb.tolist(), n_results=2)
        docs = results.get("documents", [[]])[0]

        if docs:
            context = " ".join(docs)
        else:
            context = ""

        # 2. Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ â†’ Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
        if not context.strip():
            st.write("Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ØŒ Ø£Ø¨Ø­Ø« Ø§Ù„Ø¢Ù† ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ...")
            web_results = search_internet(user_input)
            context = web_results

        # 3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {user_input}\n\nØ§Ù„Ù…Ø±Ø§Ø¬Ø¹:\n{context}\n\nØ£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø¨Ø¥ÙŠØ¬Ø§Ø² ÙˆØ¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ø¶Ø­ Ù„Ù„Ø·Ù„Ø§Ø¨."
        answer = query_huggingface(prompt)

        st.success("âœï¸ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        st.write(answer)
