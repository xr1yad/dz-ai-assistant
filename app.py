import streamlit as st
import requests
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import os
import numpy as np

# ================= Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© =================
st.set_page_config(page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ", layout="wide")

st.title("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø§Ù„Ø°ÙƒÙŠ")
st.write("Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø£Ùˆ Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ğŸ‡©ğŸ‡¿")

# Ù…ÙØªØ§Ø­ Serper API (Ø¶Ø¹ Ù…ÙØªØ§Ø­Ùƒ Ù‡Ù†Ø§ ğŸ‘‡)
SERPER_API_KEY = "0fbd7aac9c335c9b56d7b2acfe40253bfe34f614"

# Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ù„ØºÙˆÙŠ Ù…Ù† Hugging Face (Ù…Ø¬Ø§Ù†ÙŠ)
model = pipeline("text-generation", model="microsoft/phi-2", max_new_tokens=200)

# Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬
DB_DIR = "chroma_db"
os.makedirs(DB_DIR, exist_ok=True)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
client = chromadb.Client(Settings(persist_directory=DB_DIR))
try:
    collection = client.get_collection("curriculum")
except:
    collection = client.create_collection("curriculum")

# ================= ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª =================
def search_internet(query):
    url = "https://google.serper.dev/search"
    headers = {"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"}
    payload = {
        "q": f"{query} site:education.gov.dz OR site:onefd.edu.dz OR site:wikipedia.org OR site:britannica.com",
        "num": 5
    }
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        data = response.json()
        results = []
        for item in data.get("organic", []):
            results.append(f"{item.get('title')} - {item.get('link')}")
        return "\n".join(results)
    else:
        return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø¢Ù†."

# ================= ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… =================
user_input = st.chat_input("ğŸ—¨ï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

if user_input:
    with st.chat_message("user"):
        st.write(user_input)

    with st.chat_message("assistant"):
        st.write("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...")

        # 1. Ø§Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©
        q_emb = embed_model.encode([user_input])
        results = collection.query(query_embeddings=q_emb.tolist(), n_results=2)
        docs = results.get("documents", [[]])[0]

        if docs:
            context = " ".join(docs)
        else:
            context = ""

        # 2. Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ â†’ Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
        if not context.strip():
            st.write("Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ØŒ Ø£Ø¨Ø­Ø« Ø§Ù„Ø¢Ù† ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ...")
            web_results = search_internet(user_input)
            context = web_results

        # 3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {user_input}\n\nØ§Ù„Ù…Ø±Ø§Ø¬Ø¹:\n{context}\n\nØ£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø¨Ø¥ÙŠØ¬Ø§Ø² ÙˆØ¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ø¶Ø­ Ù„Ù„Ø·Ù„Ø§Ø¨."
        answer = model(prompt)[0]['generated_text']

        st.success("âœï¸ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        st.write(answer)
